# modules/generator.py

from typing import Optional
from pathlib import Path
import json
from openai import OpenAI
from config import settings
from modules.utils import logger, validate_json
from modules.retriever import MedicalRetrieverOnline
from modules.retriever import MedicalRetrieverOffline
from typing import Optional, List, Dict

client_deepseek = OpenAI(
    base_url=settings.DEEPSEEK_BASE_URL,
    api_key=settings.DEEPSEEK_API_KEY
)

def get_response(messages, model="deepseek-ai/DeepSeek-V3", temperature=0.7):
    resp = client_deepseek.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature
    )
    return resp.choices[0].message.content

class DeepSeekGenerator:
    @staticmethod
    def generate_answer(
        query: str,
        dialogue_history: List[Dict], 
        top_k: int = 50,
        model: str = "deepseek-ai/DeepSeek-V3",
        temperature: float = 0.7
    ) -> Optional[str]:
        try:
            fallback_json = {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•æä¾›æœ‰æ•ˆå»ºè®®ã€‚",
                "suggestion": "è¯·å°è¯•æè¿°å¾—æ›´è¯¦ç»†ä¸€äº›ï¼Œæˆ–è€…ç¨åå†è¯•ã€‚",
                "risk_level": "æœªçŸ¥",
                "possible_causes": [],
                "recommended_department": "æ— "
            }
            # 1. ç”¨æ£€ç´¢å™¨æŒ‘å‡º top_k æ¡æœ€ç›¸å…³çš„ contexts
            retriever1 = MedicalRetrieverOnline()
            retriever2 = MedicalRetrieverOffline()
            docs1 = retriever1.hybrid_retrieve(query, top_k=top_k)
            docs2 = retriever2.hybrid_retrieve(query, top_k=top_k)
            contexts1 = [doc.page_content for doc in docs1]
            contexts2 = [doc.page_content for doc in docs2]
            # contexts2 = []
            # 2. æ„å»ºå¸¦å†å²ä¸Šä¸‹æ–‡çš„prompt
            joined_contexts1 = "\n".join(f"- {ctx}" for ctx in contexts1)
            joined_contexts2 = "\n".join(f"- {ctx}" for ctx in contexts2)
            system_content = (
                "ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç–—åŠ©ç†ï¼Œæ­£åœ¨ä¸ç”¨æˆ·è¿›è¡Œå¤šè½®å¯¹è¯ã€‚\n"
                "è¯·ç»“åˆä»¥ä¸‹åŒ»å­¦èµ„æ–™å’Œå†å²å¯¹è¯è®¤çœŸå›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n\n"
                "ã€ç½‘é¡µæœç´¢ä¿¡æ¯ã€‘\n"
                f"{joined_contexts1}\n\n"
                "ã€æƒå¨æ‰‹å†Œä¿¡æ¯ã€‘\n"
                f"{joined_contexts2}\n\n"
                "è¯·ä»¥ä»¥ä¸‹ JSON è¾“å‡ºï¼Œä¸å¾—æ·»åŠ  JSON å¤–æ–‡å­—ï¼š\n"
                "{\n"
                '  "direct_reply": "",\n'
                '  "answer": "",\n'
                '  "suggestion": "",\n'
                '  "risk_level": "ä½ / ä¸­ / é«˜",\n'
                '  "confidence": 0.0,\n'
                '  "consult_urgency": "ç«‹å³å°±åŒ» / 48h å†… / è§‚å¯Ÿå³å¯",\n'
                '  "possible_causes": ["..."],\n'
                '  "recommended_department": "",\n'
                "}\n"
                "âš ï¸ possible_causes å’Œ references å¿…é¡»æ˜¯ **åŒå¼•å·** åŒ…è£¹çš„ JSON æ•°ç»„ã€‚"
            )
            
            system_content += (
                "\n\nè¯·å°† possible_causes å­—æ®µæ‰©å±•ä¸ºå¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š\n"
                '  â€¢ "name"ï¼šç–¾ç—…åç§°\n'
                '  â€¢ "reason"ï¼šä¸ºä»€ä¹ˆæ€€ç–‘å®ƒï¼ˆå…¸å‹ç—‡çŠ¶/æµè¡Œç—…å­¦ï¼‰\n'
                '  â€¢ "test"ï¼šé¦–é€‰æ’æŸ¥æ–¹å¼(å½±åƒ/å®éªŒå®¤/ä½“æ ¼)\n'
                "ç¤ºä¾‹ï¼š\n"
                '"possible_causes": [\n'
                '  {"name":"è‚ºç‚","reason":"ä½çƒ§+å’³å—½+æ¹¿å•°éŸ³","test":"èƒ¸ç‰‡"},\n'
                '  {"name":"è‚ºç»“æ ¸","reason":"ä½çƒ­ç›—æ±—ä½“é‡å‡è½»","test":"èƒ¸ç‰‡/ç—°æ¶‚ç‰‡"}\n'
                "]\n"
                "è¯·ç¡®ä¿è¿”å›å†…å®¹ä¸¥æ ¼ä¸º JSON æ ¼å¼ï¼Œå¼€å¤´å¿…é¡»æ˜¯ {ï¼Œä¸å¾—åŒ…å«å¤šä½™å¥å­æˆ–æ ‡ç‚¹ã€‚"
            )



            # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºã€å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼‰
            messages = [{"role": "system", "content": system_content}]
            
            # æ·»åŠ å†å²å¯¹è¯è®°å½•
            for entry in dialogue_history:
                messages.append({
                    "role": entry["role"],
                    "content": entry["content"]
                })
            
            # æ·»åŠ å½“å‰é—®é¢˜
            messages.append({"role": "user", "content": query})

            # 4. è°ƒç”¨ç”Ÿæˆæ¥å£ â€”â€” åŠ é‡è¯•
            attempt = 0
            temp = temperature
            while attempt < 3:
                result = get_response(messages, model=model, temperature=temp)
                logger.info("ğŸŸ¢ åŸå§‹æ¨¡å‹è¾“å‡º:\n%s", result)
                if validate_json(result):
                    return result
                attempt += 1
                temp = max(0.5, temp - 0.2)

            # ä¸‰æ¬¡éƒ½å¤±è´¥
            return json.dumps(fallback_json)

            

        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return json.dumps(fallback_json)
        
    @staticmethod
    def stream_answer(
        messages,
        model: str = "deepseek-ai/DeepSeek-V3",
        temperature: float = 0.7,
    ):
        """
        æŒ‰ token æµå¼äº§å‡ºï¼Œç”¨äºæ‰“å­—æœºæ•ˆæœã€‚
        è°ƒç”¨æ–¹å¼ä¸ generate_answer ä¿æŒåŒä¸€ messages ç»“æ„ã€‚
        """
        resp = client_deepseek.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,          # å…³é”®å‚æ•°
        )
        for chunk in resp:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    @staticmethod
    def stream_natural_reply(
        query: str,
        dialogue_history,
        model: str = "deepseek-ai/DeepSeek-V3",
        temperature: float = 0.7,
    ):
        """
        ä¸ stream_generate_answer ç›¸åŒæ£€ç´¢ï¼Œä½†è¦æ±‚æ¨¡å‹åªç”¨è‡ªç„¶è¯­è¨€å›å¤ã€‚
        """
        # --- æ£€ç´¢ä¸ system_content ä¸ stream_generate_answer ç›¸åŒ ---
        retriever1 = MedicalRetrieverOnline()
        retriever2 = MedicalRetrieverOffline()
        ctx1 = "\n".join(f"- {d.page_content}"
                         for d in retriever1.hybrid_retrieve(query, top_k=20))
        ctx2 = "\n".join(f"- {d.page_content}"
                         for d in retriever2.hybrid_retrieve(query, top_k=20))

        system_content = (
            "ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç–—åŠ©ç†ï¼Œæ­£åœ¨è¿›è¡Œå¤šè½®å¯¹è¯ã€‚\n"
            "ä»¥ä¸‹ä¿¡æ¯ä¾›å‚è€ƒï¼ˆè¯·å‹¿ç›´æ¥å¼•ç”¨åŸæ–‡ï¼‰ï¼š\n"
            f"{ctx1}\n{ctx2}\n"
            "è¯·**ä»…ç”¨ä¸­æ–‡è‡ªç„¶è¯­è¨€**å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•èŠ±æ‹¬å·ã€å¼•å·æˆ– JSON æ ¼å¼æ ‡è®°ã€‚"
        )

        messages = [{"role": "system", "content": system_content}]
        messages += dialogue_history
        messages.append({"role": "user", "content": query})

        resp = client_deepseek.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,
        )
        for chunk in resp:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


